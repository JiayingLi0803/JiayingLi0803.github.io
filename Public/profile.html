<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- import front-page.css-->
    <link rel="stylesheet" href="profile.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
    <title>Jiaying Li | Profile</title>
</head>

<body>
    <div class="navbar">
        <ul>
            <li class="brand">Jiaying Li</li>
            <li class="navtext"><a href="#">About</a></li>
            <li class="navtext"><a href="#">Research</a></li>
            <li class="navtext"><a href="#">Publication</a></li>
            <li class="navtext"><a href="#">CV</a></li>
        </ul>
    </div>
    <div class="underBlock">
        <div class="aboutBlock">
            <div class="contactBlock">
                <img src="../Home/photo.png" style="width: 200px;" class="profilePhoto">
                <p class="nameBold">Jiaying Li</p>
                <p class="textcontent">Master's student in Music Technology<br>
                    Georgia Institute of Technology<br>
                    jli3269@gatech.edu</p>
                <a href="https://www.linkedin.com/in/jiayingli0803/" target="_blank"><i
                        class="fab fa-linkedin fa-lg"></i></a>
                <a href="https://github.com/JiayingLi0803" target="_blank"><i class="fab fa-github fa-lg"></i></a>
            </div>
            <div class="introBlock">
                <p class="textcontent">I am Jiaying Li, a second-year master's student in music technology at Georgia
                    Tech. I work as a
                    researcher in the Computational and Cognitive Musicology Lab (CCML) at the <a
                        href="https://music.gatech.edu/research">Georgia Tech Center for Music Technology (GTCMT)</a>
                    under the guidance of <a href="https://music.gatech.edu/nat-condit-schultz">Dr. Nat
                        Condit-Schultz</a> and <a href="https://music.gatech.edu/claire-arthur">Dr. Arthur Claire</a>.
                    My
                    research focuses on music cognition and natural language processing. I proposed a music similarity
                    algorithm based on the hidden
                    Markov model.
                    Currently, I am working on a research project involving singing voice conversion using a wav2wav
                    deep learning model with the LSTM network.</p>

                <p class="textcontent">I completed my Bachelor of Engineering degree in <a
                        href="https://sse.cuhk.edu.cn/en/page/539">Electronic Information Engineering</a>
                    and minor in <a href="https://ge.cuhk.edu.cn/en/page/37">Philosophy</a> at <a
                        href="https://www.cuhk.edu.cn/en">the Chinese University
                        of Hong Kong, Shenzhen</a>. During my undergraduate studies, I worked at <a
                        href="https://airs.cuhk.edu.cn/en">Shenzhen
                        Institute of Artificial Intelligence and Robotics for Society</a>, where my focus was on
                    developing an
                    interactive auto music generation system.</p>

                <p class="textcontent">Outside of academics, I have a passion for playing the piano and writing novels
                    in my spare time.
                    Also, I am a professional speed cuber.</p>
            </div>
        </div>
        <div class="blankBlock"></div>
        <div class="pubBlock">
            <h1 class="h1title">Publication</h1>
            <h2 class="h2title">Conference</h2>
            <p>K. Xue, Z. Liu, <strong>J. Li</strong>, X. Ji and H. Qian, "SongBot: An Interactive Music Generation Robotic System for Non-musicians Learning from A Song," <i>2021 IEEE International Conference on Real-time Computing and Robotics (RCAR)</i>, Xining, China, 2021, pp. 1300-1305, doi: 10.1109/RCAR52367.2021.9517454. <a href="Files/songbot.pdf">[PDF]</a></p>
            <h2 class="h2title">Poster</h2>
            <p><strong>J. Li</strong> and N. Condit-Schultz, “Four Chords Go a Long Way: Measuring Chord Progression Similarity in Chinese Popular Music”, <i>2022 Society for Music Perception and Cognition (SMPC)</i>. <a href="Files/SMPC.pdf">[Poster]</a></p>
        </div>
        <div class="blankBlock"></div>
        <div class="researchBlock">
            <h1 class="h1title">Research</h1>
            <h2 class="h2title">Active Projects</h2>
            <div class="stream">
                <div class="thumbnailfig">
                    <img class="thumbnailfig" src="Figures/raproject.png" style="width:200px">
                </div>
                <div class="projectbody">
                    <h3 class="h3title">Audio Technology II Interaction Website <a href="https://jiayingli0803.github.io/AudioTechII_GRA/interaction/interaction.html">[Website]</a></h3>
                    <p>Collaborator: Dr. Arthur Claire<br>
                        Georgia Tech Center for Music Technology</p>
                    <p class="textcontent">This research project addresses the challenges encountered by students without an engineering background when trying to comprehend the fundamental concepts of digital signal processing (DSP). Concepts such as convolution, autocorrelation, modulation, and the discrete Fourier transform (DFT) often prove difficult to grasp without proper visualization and practical application. To overcome these difficulties and enhance comprehension, we have developed a website featuring interactive modules that allow students to visualize animations and simulations of these DSP concepts. Additionally, the website offers coding practices and exercises specifically designed to assist beginners in acquiring essential audio processing skills. By actively engaging with these interactive tools and practical exercises, students are encouraged to delve deeper into the learning process, resulting in a more profound understanding of DSP principles. The website is implemented using HTML, CSS, and JavaScript, and it is intended for integration into the Audio Technology II lecture curriculum in Fall 2023.</p>
                </div>
            </div>
            <h2 class="h2title">Past Projects</h2>
            <div class="stream">
                <div class="thumbnailfig">
                    <img class="thumbnailfig" src="Figures/svc.png" style="width:200px">
                </div>
                <div class="projectbody">
                    <h3 class="h3title">Singing Voice Conversion based on Target Waveform Mapping <a href="Files/SVCposter.pdf">[Poster]</a></h3>
                    <p>Collaborator: Dr. Nat Condit-Schultz<br>
                        Georgia Tech Center for Music Technology</p>
                    <p class="textcontent">Singing voice conversion (SVC) is a technique that converts the source sound of singer A to the target sound of singer B without changing the lyric contents. In this paper, we implemented and validated our baseline model, the MelGAN model, which is inspired by the voice conversion (VC) task. We trained the model on an existing database, NUS-48 collected by the National University of Singapore. In our research, we found that the vocal range has a great impact on the model training results. Therefore, we collected our own database and purposed a more intuitive method, the waveform mapping method. We compared the results with different training data and found that the database with a larger frequency range will provide a better conversion result. Also, the waveform mapping method works better on specific target timbres and source timbres. The main contribution of this research project is the new dataset, and also our idea of waveform mapping algorithm. These findings provide new insights into the singing voice conversion task.</p>
                </div>
            </div>
            <div class="stream">
                <div class="thumbnailfig">
                    <img class="thumbnailfig" src="Figures/cpsi.png" style="width:200px">
                </div>
                <div class="projectbody">
                    <h3 class="h3title">A Chord Progression Similarity Calculation Method Adapted to Human Ear Perception <a href="Files/CPSI.pdf">[PDF]</a></h3>
                    <p>Collaborator: Dr. Nat Condit-Schultz<br>
                        Georgia Tech Center for Music Technology</p>
                    <p class="textcontent">This research presents a novel method, the Chord Progression Similarity Index (CPSI), for quantifying the similarity between chord progressions. The CPSI measures the sum of 1st-order transitions between pitch classes in consecutive chords. The algorithm allows for customization by incorporating weights for important metric positions, chord degrees, and other factors. The effectiveness of the CPSI is evaluated through two experiments. In a perceptual experiment, 34 participants rated the perceived similarity of 40 diatonic chord progressions. Various CPSI calculations were compared to the participants' responses, revealing a weak correlation. Notably, the participants' responses were most accurately predicted by considering only root progression. In a separate computational experiment, the CPSI was employed to assess chord progression diversity within a novel database comprising 200 Chinese songs, representing the most popular selections from each year between 2012 and 2021. The findings provide evidence indicating an increasing similarity in Chinese pop music over recent years, aligning with numerous existing reports. This research showcases the utility of the CPSI in quantifying chord progression similarities and contributes insights into the evolving characteristics of Chinese pop music.</p>
                </div>
            </div>
            <div class="stream">
                <div class="thumbnailfig">
                    <img class="thumbnailfig" src="Figures/paintInstrument.png" style="width:200px">
                </div>
                <div class="projectbody">
                    <h3 class="h3title">An Interactive Music Generation Software Through Painting <a href="https://github.com/JiayingLi0803/PaintingInstrument">[Codes]</a></h3>
                    <p>Georgia Tech Center for Music Technology</p>
                    <p class="textcontent">The musical painting software allows people to draw and generate the corresponding music based on the built-in rules. By selecting brushes and painting on the canvas, people can generate their real-time music.</p>
                </div>
            </div>
            <div class="stream">
                <div class="thumbnailfig">
                    <img class="thumbnailfig" src="Figures/cube.png" style="width:200px">
                </div>
                <div class="projectbody">
                    <h3 class="h3title">An Innovative Musical Instrument Prototype Based on Rubik's Cube: Musical Cube <a href="Files/musicCube.pdf">[PDF]</a></h3>
                    <p>Georgia Tech Center for Music Technology</p>
                    <p class="textcontent">The Musical Cube is a groundbreaking prototype that combines a traditional Rubik's cube with music-sounding software. It generates chords and melodies based on the cube's rotation, creating music that corresponds to its scrambled state. This innovative instrument serves as both a tool for Rubik's Cube beginners to learn solution algorithms and an improvisation tool for musicians. Evaluation results from performers and audiences highlight its potential and future prospects. The Musical Cube offers a unique fusion of technology, music, and puzzle-solving, providing an engaging and interactive musical experience for enthusiasts and musicians alike.</p>
                </div>
            </div>
            <div class="stream">
                <div class="thumbnailfig">
                    <img class="thumbnailfig" src="Figures/goggles2.png" style="width:200px">
                </div>
                <div class="projectbody">
                    <h3 class="h3title">DJI Goggles 2 <a href="https://www.dji.com/goggles-2">[Product]</a></h3>
                    <p>Collaborators: DJI Goggles 2 Group<br>
                        SZ DJI Technology CO., LTD. </p>
                    <p class="textcontent">I have worked as a User Experience Research Scientist at DJI, where I actively contributed to the design process of DJI Goggles 2. As part of my role, I conducted competitor analysis focusing on AR and VR products, aiming to gain insights and inform our design decisions. To enhance the user experience of DJI Avata and DJI Goggles 2, I conducted perceptual experiments that yielded valuable findings. Additionally, I took charge of designing ergonomics experiments aimed at improving the shape and weight of the DJI Goggles mask, and effectively analyzed optical data to optimize the qualifying range for our products. The outcomes of these endeavors were highly favorable, demonstrating that DJI Goggles 2 is an exceptional product.</p>
                </div>
            </div>
            <div class="stream">
                <div class="thumbnailfig">
                    <img class="thumbnailfig" src="Figures/fbp.jpg" style="width:200px">
                </div>
                <div class="projectbody">
                    <h3 class="h3title">Facial Beauty Scoring Recommendations Based on VGG16 and XGboost</h3>
                    <p>Collaborators: <a href="https://scholar.google.com/citations?user=IOagLnEAAAAJ&hl=en">David Zhang</a>, Lynn Liao<br>
                        Shenzhen Institute of Artificial Intelligence and Robotics for Society</p>
                    <p class="textcontent">Facial attractiveness significantly impacts various aspects of social interactions, including entertainment, career prospects, and everyday life. To investigate the relationship between facial beauty and its influence, we constructed two extensive databases: a Celebrity Faces database comprising more than 4,900 well-known personalities, and a human face database containing over 6,000 samples obtained from Hefei University. Leveraging the VGG16 model, we extracted the contour features of eyes and eyebrows, subsequently developing a system that utilizes the XGBoost model for predicting facial beauty degrees. Our research contributes to the understanding of facial aesthetics by providing a robust framework for evaluating attractiveness based on facial contour features.</p>
                </div>
            </div>




            

            

            <div class="stream">
                <div class="projectbody">
                    <h3 class="h3title"></h3>
                </div>
                <div class="thumbnailfig">

                </div>
            </div>
        </div>
    </div>
    <!-- Rest of your content -->
</body>

</html>