<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-8CHBXR3Y5Z"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-8CHBXR3Y5Z');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- import front-page.css-->
    <link rel="shortcut icon" href="Home/rabbit.ico">
    <link rel="stylesheet" href="Public/Profile/profile.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
    <title>Jiaying Li | Profile</title>
</head>



<body>
    <div class="navbar">
        <h1 class="brand">Jiaying Li</h1>
        <input type="checkbox" id="nav-checkbox"><label for="nav-checkbox"></label>
        <ul>
            <!-- <li><a href="Public/Profile/about.html">About</a></li>
            <li><a href="Public/Profile/pulication.html">Publication</a></li>
            <li><a href="Public/Profile/research.html">Research</a></li> -->
            <!-- <li><a href="Public/Profile/cv.html">CV</a></li> -->
            <li><a href="#about">About</a></li>
            <li><a href="#publication">Publication</a></li>
            <li><a href="#research">Research</a></li>
            <li><a href="Public/Files/Portfolio_JiayingLi.pdf">Portfolio</a></li>

        </ul>
    </div>
    <div class="underBlock" id="about">
        <section id="page-about">
            <div class="aboutBlock">
                <div class="contactBlock">
                    <img src="Home/photo.png" style="width: 200px;" class="profilePhoto">
                    <div class="contact-text">
                        <p class="nameBold">Jiaying Li</p>
                        <p class="textcontent">Master's Student in Music Technology<br>
                            Georgia Institute of Technology<br>
                            jli3269@gatech.edu</p>
                    </div>
                    <div class="contact-icons">
                        <a href="https://www.linkedin.com/in/jiayingli0803/" target="_blank"><i class="fab fa-linkedin fa-lg"></i></a>
                        <a href="https://github.com/JiayingLi0803" target="_blank"><i class="fab fa-github fa-lg"></i></a>
                        <a href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=Z-r1Pi8AAAAJ"><i class="fa fa-graduation-cap fa-lg"></i></a>
                        <a href="https://twitter.com/jli3269" target="_blank"><i class="fab fa-twitter fa-lg"></i></a>
                    </div>
                </div>
                <div class="introBlock">
                    <p class="textcontent">I am Jiaying Li, a second-year master's student in music technology at Georgia Tech. I work as a researcher in the <a href="https://ccml.gtcmt.gatech.edu/team/">Computational and Cognitive Musicology Lab (CCML)</a> at the <a href="https://music.gatech.edu/research">Georgia Tech Center for Music Technology (GTCMT)</a> under the guidance of <a href="https://music.gatech.edu/nat-condit-schultz">Dr. Nat Condit-Schultz</a> and <a href="https://music.gatech.edu/claire-arthur">Dr. Claire Arthur</a>. My research at Georgia Tech focuses on Human-Computer Interaction (HCI). During my first year of master's studies, my research focused on music cognition & perception and audio signal processing. Progressing into the second year, my research interest shifted to Human-Computer Interaction (HCI) and Visualization.</p>

                    <p class="textcontent">I completed my Bachelor of Engineering degree in <a href="https://sse.cuhk.edu.cn/en/page/539">Electronic Information Engineering</a> and minor in <a href="https://ge.cuhk.edu.cn/en/page/37">Philosophy</a> at <a
href="https://www.cuhk.edu.cn/en">the Chinese University of Hong Kong, Shenzhen</a>. During my undergraduate studies, I worked at <a href="https://airs.cuhk.edu.cn/en">Shenzhen Institute of Artificial Intelligence and Robotics for Society (AIRS)</a>, where my focus was on developing an interactive auto music generation system and facial beauty prediction.</p>

                    <p class="textcontent">Outside of academics, I have a passion for playing the piano and writing novels in my spare time. Also, I am a professional speed cuber.</p>
                </div>
            </div>
        </section>



        <div class="blankBlock"></div>


        <section id="page-publication">
            <div class="pubBlock" id="publication">
                <h1 class="h1title">Publication</h1>
                <h2 class="h2title">Conference Paper</h2>
                <p>K. Xue, Z. Liu, <strong>J. Li</strong>, X. Ji and H. Qian, "SongBot: An Interactive Music Generation Robotic System for Non-musicians Learning from A Song," <i>2021 IEEE International Conference on Real-time Computing and Robotics (RCAR)</i>, Xining, China, 2021, pp. 1300-1305, doi: 10.1109/RCAR52367.2021.9517454. <a href="Public/Files/songbot.pdf">[PDF]</a><a href="Public/Videos/exp_cut.mp4">[Video]</a></p>
                <h2 class="h2title">Conference Poster</h2>
                <p><strong>J. Li</strong> and N. Condit-Schultz, “Four Chords Go a Long Way: Measuring Chord Progression Similarity in Chinese Popular Music”, <i>2022 Society for Music Perception and Cognition (SMPC)</i>.
                    <a href="Public/Files/SMPC.pdf">[Poster]</a></p>
            </div>
        </section>

        <div class="blankBlock"></div>

        <section id="page-research">
            <div class="researchBlock" id="research">
                <h1 class="h1title">Research</h1>
                <h2 class="h2title">Active Projects</h2>

                <div class="stream">
                    <div class="thumbnailfig">
                        <img class="thumbnailimg" src="Public/Figures/typing.png">
                    </div>
                    <div class="projectbody">
                        <h3 class="h3title">Mid Air Text Interaction with Hand-Tracking</h3>
                        <p>
                            <!-- <a
                        href="Public/Videos/AlvinModel.mp4">[Video]</a> -->
                        </p>
                        <p>Collaborator: <a href="https://ivi.cc.gatech.edu/people.html">Dr. Yalong Yang</a><br>
                            <a href="https://ivi.cc.gatech.edu/">Georgia Tech Immersive Visualization & Interaction Lab</a>
                        </p>


                        <p class="textcontent">Our goal is to develop a ten-finger text input prototype in virtual reality based on hand tracking. The project includes text input, text selection, and some basic operations such as copy/cut/paste.</p>
                    </div>
                </div>

                <!-- <hr class="hr-edge-weak"><br>

                <div class="stream">
                    <div class="thumbnailfig">
                        <img class="thumbnailimg" src="Public/Figures/separation.png">
                    </div>
                    <div class="projectbody">
                        <h3 class="h3title">Target Speech Separation</h3>
                        <p>Personal Research Project</p>
                        <p class="textcontent">This research project aims at separating speech from singing voice by using deep learning models. I've finished the literature review and already started working on the dataset and pipeline.</p>
                    </div>
                </div> -->
                <hr class="hr-twill">
                <div class="sepblank"></div>

                <h2 class="h2title">Past Projects</h2>

                <div class="stream">
                    <div class="thumbnailfig">
                        <img class="thumbnailimg" src="Public/Figures/alvin.png">
                    </div>
                    <div class="projectbody">
                        <h3 class="h3title">WHOI ALVIN Submersible</h3>
                        <p>
                            <a
                        href="https://drive.google.com/file/d/1ExDLO9RvysJCSCjuciA8N0lb6gj34FEL/view?usp=sharing">[Video]</a>
                        </p>
                        <p>Collaborator: <a href="http://sonify.psych.gatech.edu/~walkerb/">Dr. Bruce Walker</a>, Angela Dai<br>
                            <a href="http://sonify.psych.gatech.edu/">Georgia Tech Sonification Lab</a>
                        </p>


                        <p class="textcontent">This project aims to create an immersive orientation and a training workflow for the WHOI ALVIN submersible using Virtual Reality (VR). During the orientation, users should be able to follow the instructions and see the highlights of all the interior panels and objects. Also, they will be able to interact with the interior objects, for example, multimeters, buttons and switches, with controllers. For the safety training workflow, users are taught some safety instructions, for example, how to use the fire extinguisher, how to deal with the situation when the submersible is stuck, how to throw away the trash, etc. The simulation system is built in Unity and tested by the Meta Quest 2 and Meta Quest 3.</p>
                    </div>
                </div>

                <hr class="hr-edge-weak"><br>

                <div class="stream">
                    <div class="thumbnailfig">
                        <img class="thumbnailimg" src="Public/Figures/raproject.png">
                    </div>
                    <div class="projectbody">
                        <h3 class="h3title">Audio Technology II Interaction Website</h3>
                        <p>
                            <a href="https://jiayingli0803.github.io/AudioTechII_GRA/interaction/interaction.html">[Website]</a>
                            <a href="https://github.com/JiayingLi0803/AudioTechII_GRA">[Code]</a>
                        </p>
                        <p>Collaborator: Dr. Claire Arthur<br>
                            Georgia Tech Center for Music Technology</p>
                        <p class="textcontent">This research project addresses the challenges encountered by students without an engineering background when trying to comprehend the fundamental concepts of digital signal processing (DSP). Concepts such as convolution, autocorrelation, modulation, and the discrete Fourier transform (DFT) often prove difficult to grasp without proper visualization and practical application. To overcome these difficulties and enhance comprehension, we have developed a website featuring interactive modules that allow students to visualize animations and simulations of these DSP concepts. Additionally, the website offers coding practices and exercises specifically designed to assist beginners in acquiring essential audio processing skills. By actively engaging with these interactive tools and practical exercises, students are encouraged to delve deeper into the learning process, resulting in a more profound understanding of DSP principles. The website is implemented using HTML, CSS, and JavaScript, and it is intended for integration into the Audio Technology II lecture curriculum in Fall 2023.</p>
                    </div>
                </div>

                <hr class="hr-edge-weak"><br>

                <div class="stream">
                    <div class="thumbnailfig">
                        <img class="thumbnailimg" src="Public/Figures/access.png">
                    </div>
                    <div class="projectbody">
                        <h3 class="h3title">Accessible Learning Material User Interface Prototype for Disabled Students</h3>
                        <p>
                            <a href="Public/Files/AccessAbilityFinalAnalysis.pdf">[PDF]</a>
                            <a href="https://github.com/JiayingLi0803/Access-Ability-LearningMaterial">[Code]</a>
                        </p>
                        <p>Advisor: Dr. Michael Helms</p>
                        <p class="textcontent">Our project aims at helping designers to design more accessible course materials for visually impaired students. We have sought advice from developers of existing apps or websites and professional therapists to comprehend how these students learn and cognize new knowledge. The data we gather from the interviews can be used to improve existing computational cognitive models. Based on our interview findings, we can outline the design schema for our computational cognitive model, which incorporates critical considerations and strategies for creating accessible learning tools for visually impaired students.</p>
                    </div>
                </div>

                <hr class="hr-edge-weak"><br>

                <div class="stream">
                    <div class="thumbnailfig">
                        <img class="thumbnailimg" src="Public/Figures/svc.png">
                    </div>
                    <div class="projectbody">
                        <h3 class="h3title">Singing Voice Conversion based on Target Waveform Mapping</h3>
                        <p>
                            <a href="Public/Files/SVCposter.pdf">[Poster]</a>
                            <a href="https://github.com/JiayingLi0803/SingingVoiceConversion">[Code]</a>
                        </p>
                        <p>Collaborator: Dr. Nat Condit-Schultz<br>
                            Georgia Tech Center for Music Technology</p>
                        <p class="textcontent">Singing voice conversion (SVC) is a technique that converts the source sound of singer A to the target sound of singer B without changing the lyric contents. In this paper, we implemented and validated our baseline model, the MelGAN model, which is inspired by the voice conversion (VC) task. We trained the model on an existing database, NUS-48 collected by the National University of Singapore. In our research, we found that the vocal range has a great impact on the model training results. Therefore, we collected our own database and purposed a more intuitive method, the waveform mapping method. We compared the results with different training data and found that the database with a larger frequency range will provide a better conversion result. Also, the waveform mapping method works better on specific target timbres and source timbres. The main contribution of this research project is the new dataset, and also our idea of waveform mapping algorithm. These findings provide new insights into the singing voice conversion task.</p>
                    </div>
                </div>

                <hr class="hr-edge-weak"><br>

                <div class="stream">
                    <div class="thumbnailfig">
                        <img class="thumbnailimg" src="Public/Figures/cpsi.png">
                    </div>
                    <div class="projectbody">
                        <h3 class="h3title">A Chord Progression Similarity Calculation Method Adapted to Human Ear Perception</h3>
                        <p>
                            <a href="Public/Files/CPSI.pdf">[PDF]</a>
                            <a href="https://github.com/JiayingLi0803/harmonicSimilarityProject_2022">[Code]</a>
                        </p>
                        <p>Collaborator: Dr. Nat Condit-Schultz<br>
                            Georgia Tech Center for Music Technology</p>
                        <p class="textcontent">This research presents a novel method, the Chord Progression Similarity Index (CPSI), for quantifying the similarity between chord progressions. The CPSI measures the sum of 1st-order transitions between pitch classes in consecutive chords. The algorithm allows for customization by incorporating weights for important metric positions, chord degrees, and other factors. The effectiveness of the CPSI is evaluated through two experiments. In a perceptual experiment, 34 participants rated the perceived similarity of 40 diatonic chord progressions. Various CPSI calculations were compared to the participants' responses, revealing a weak correlation. Notably, the participants' responses were most accurately predicted by considering only root progression. In a separate computational experiment, the CPSI was employed to assess chord progression diversity within a novel database comprising 200 Chinese songs, representing the most popular selections from each year between 2012 and 2021. The findings provide evidence indicating an increasing similarity in Chinese pop music over recent years, aligning with numerous existing reports. This research showcases the utility of the CPSI in quantifying chord progression similarities and contributes insights into the evolving characteristics of Chinese pop music.</p>
                    </div>
                </div>

                <hr class="hr-edge-weak"><br>

                <div class="stream">
                    <div class="thumbnailfig">
                        <img class="thumbnailimg" src="Public/Figures/paintInstrument.png">
                    </div>
                    <div class="projectbody">
                        <h3 class="h3title">An Interactive Music Generation Software Through Painting</h3>
                        <p>
                            <a href="https://github.com/JiayingLi0803/PaintingInstrument">[Code]</a>
                        </p>
                        <p>Georgia Tech Center for Music Technology</p>
                        <p class="textcontent">The musical painting software allows people to draw and generate the corresponding music based on the built-in rules. By selecting brushes and painting on the canvas, people can generate their real-time music. The project source code was written in Python.</p>
                    </div>
                </div>

                <hr class="hr-edge-weak"><br>

                <div class="stream">
                    <div class="thumbnailfig">
                        <img class="thumbnailimg" src="Public/Figures/cube.png">
                    </div>
                    <div class="projectbody">
                        <h3 class="h3title">An Innovative Musical Instrument Prototype Based on Rubik's Cube: Musical Cube</h3>
                        <p>
                            <a href="Public/Files/musicCube.pdf">[PDF]</a>
                            <a href="https://github.com/JiayingLi0803/musicalCube">[Code]</a>
                        </p>
                        <p>Georgia Tech Center for Music Technology</p>
                        <p class="textcontent">The Musical Cube is a groundbreaking prototype that combines a traditional Rubik's cube with music-sounding software. It generates chords and melodies based on the cube's rotation, creating music that corresponds to its scrambled state. This innovative instrument serves as both a tool for Rubik's Cube beginners to learn solution algorithms and an improvisation tool for musicians. Evaluation results from performers and audiences highlight its potential and future prospects. The Musical Cube offers a unique fusion of technology, music, and puzzle-solving, providing an engaging and interactive musical experience for enthusiasts and musicians alike. The project source code was written in Python. </p>
                    </div>
                </div>

                <hr class="hr-edge-weak"><br>

                <div class="stream">
                    <div class="thumbnailfig">
                        <img class="thumbnailimg" src="Public/Figures/goggles2.png">
                    </div>
                    <div class="projectbody">
                        <h3 class="h3title">DJI Goggles 2</h3>
                        <p>
                            <a href="https://www.dji.com/goggles-2">[Product]</a>
                            <a href="Public/Files/VRSpecs.pdf">[Summary of VR Specs]</a>
                        </p>
                        <p>Collaborators: DJI Goggles 2 Group<br>
                            SZ DJI Technology CO., LTD. </p>
                        <p class="textcontent">I have worked as a User Experience Research Scientist at DJI, where I actively contributed to the design process of DJI Goggles 2. As part of my role, I conducted competitor analysis focusing on AR and VR products, aiming to gain insights and inform our design decisions. To enhance the user experience of DJI Avata and DJI Goggles 2, I conducted perceptual experiments that yielded valuable findings. Additionally, I took charge of designing ergonomics experiments aimed at improving the shape and weight of the DJI Goggles mask, and effectively analyzed optical data to optimize the qualifying range for our products. The outcomes of these endeavors were highly favorable, demonstrating that DJI Goggles 2 is an exceptional product.</p>
                    </div>
                </div>

                <hr class="hr-edge-weak"><br>

                <div class="stream">
                    <div class="thumbnailfig">
                        <img class="thumbnailimg" src="Public/Figures/fbp.jpg">
                    </div>
                    <div class="projectbody">
                        <h3 class="h3title">Facial Beauty Scoring Recommendations Based on VGG16 and XGboost</h3>
                        <p>Collaborators: <a href="https://scholar.google.com/citations?user=IOagLnEAAAAJ&hl=en">David Zhang</a>, Lynn Liao<br>
                            Shenzhen Institute of Artificial Intelligence and Robotics for Society</p>
                        <p class="textcontent">Facial attractiveness significantly impacts various aspects of social interactions, including entertainment, career prospects, and everyday life. To investigate the relationship between facial beauty and its influence, we constructed two extensive databases: a Celebrity Faces database comprising more than 4,900 well-known personalities, and a human face database containing over 6,000 samples obtained from Hefei University. Leveraging the VGG16 model, we extracted the contour features of eyes and eyebrows, subsequently developing a system that utilizes the XGBoost model for predicting facial beauty degrees. Our research contributes to the understanding of facial aesthetics by providing a robust framework for evaluating attractiveness based on facial contour features.</p>
                    </div>
                </div>


                <!-- <div class="stream">
                    <div class="projectbody">
                        <h3 class="h3title"></h3>
                    </div>
                    <div class="thumbnailfig">
    
                    </div>
                </div> -->
            </div>
        </section>

    </div>
    <!-- Rest of your content -->
</body>

</html>